\section*{Benchmark design}

To evaluate the performance of our data structures, we measure the execution time and memory usage of common operations under controlled conditions. Arrays and maps are tested with a range of sizes using randomly generated values to ensure fair and representative workloads. All benchmarks are repeated multiple times to compute average costs per operation, and memory allocation is tracked via wrapper functions to monitor resource usage reliably. This approach allows comparing different implementations on an equal footing, independently of external factors such as hashing cost or system allocation behavior.
